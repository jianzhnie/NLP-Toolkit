'''
Author: jianzhnie
Date: 2021-12-24 11:11:38
LastEditTime: 2021-12-24 11:55:45
LastEditors: jianzhnie
Description:

'''

import torch
from torch import nn
from torch.nn import functional as F


class MLP(nn.Module):
    def __init__(self, num_inputs, num_hiddens, flatten):
        super().__init__()
        net = []
        net.append(nn.Dropout(0.2))
        net.append(nn.Linear(num_inputs, num_hiddens))
        net.append(nn.ReLU())
        if flatten:
            net.append(nn.Flatten(start_dim=1))
        net.append(nn.Dropout(0.2))
        net.append(nn.Linear(num_hiddens, num_hiddens))
        net.append(nn.ReLU())
        if flatten:
            net.append(nn.Flatten(start_dim=1))

        self.mlp = nn.Sequential(*net)

    def forward(self, x):
        return self.mlp(x)


class Attend(nn.Module):
    def __init__(self, num_inputs, num_hiddens, **kwargs):
        super(Attend, self).__init__(**kwargs)
        self.f = MLP(num_inputs, num_hiddens, flatten=False)

    def forward(self, A, B):
        # Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,
        # `embed_size`)
        # Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,
        # `num_hiddens`)
        f_A = self.f(A)
        f_B = self.f(B)
        # Shape of `e`: (`batch_size`, no. of tokens in sequence A,
        # no. of tokens in sequence B)
        e = torch.bmm(f_A, f_B.permute(0, 2, 1))
        # Shape of `beta`: (`batch_size`, no. of tokens in sequence A,
        # `embed_size`), where sequence B is softly aligned with each token
        # (axis 1 of `beta`) in sequence A
        beta = torch.bmm(F.softmax(e, dim=-1), B)
        # Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,
        # `embed_size`), where sequence A is softly aligned with each token
        # (axis 1 of `alpha`) in sequence B
        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)
        return beta, alpha


class Compare(nn.Module):
    """å°†ä¸€ä¸ªåºåˆ—ä¸­çš„è¯å…ƒä¸ä¸è¯¥è¯å…ƒè½¯å¯¹é½çš„å¦ä¸€ä¸ªåºåˆ—è¿›è¡Œæ¯”è¾ƒã€‚

    - ğ¯ğ´,ğ‘–=ğ‘”([ğšğ‘–,ğœ·ğ‘–]),ğ‘–=1,â€¦,ğ‘š
    - ğ¯ğµ,ğ‘—=ğ‘”([ğ›ğ‘—,ğœ¶ğ‘—]),ğ‘—=1,â€¦,ğ‘›.
    """
    def __init__(self, num_inputs, num_hiddens, **kwargs):
        super(Compare, self).__init__(**kwargs)
        self.g = MLP(num_inputs, num_hiddens, flatten=False)

    def forward(self, A, B, beta, alpha):
        V_A = self.g(torch.cat([A, beta], dim=2))
        V_B = self.g(torch.cat([B, alpha], dim=2))
        return V_A, V_B


class Aggregate(nn.Module):
    """æœ‰ä¸¤ç»„æ¯”è¾ƒå‘é‡ ğ¯ğ´,ğ‘– ï¼ˆ ğ‘–=1,â€¦,ğ‘š ï¼‰å’Œ ğ¯ğµ,ğ‘— ï¼ˆ ğ‘—=1,â€¦,ğ‘› ï¼‰ã€‚
    åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†èšåˆè¿™äº›ä¿¡æ¯ä»¥æ¨æ–­é€»è¾‘å…³ç³»ã€‚æˆ‘ä»¬é¦–å…ˆæ±‚å’Œè¿™ä¸¤ç»„æ¯”è¾ƒå‘é‡ï¼š

    - ğ¯ğ´=âˆ‘ğ‘–=1ğ‘šğ¯ğ´,ğ‘–,
    - ğ¯ğµ=âˆ‘ğ‘—=1ğ‘›ğ¯ğµ,ğ‘—.
    """
    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):
        super(Aggregate, self).__init__(**kwargs)
        self.h = MLP(num_inputs, num_hiddens, flatten=True)
        self.linear = nn.Linear(num_hiddens, num_outputs)

    def forward(self, V_A, V_B):
        # Sum up both sets of comparison vectors
        V_A = V_A.sum(dim=1)
        V_B = V_B.sum(dim=1)
        # Feed the concatenation of both summarization results into an MLP
        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))
        return Y_hat


class DecomposableAttention(nn.Module):
    def __init__(self,
                 vocab,
                 embed_size,
                 num_hiddens,
                 num_inputs_attend=100,
                 num_inputs_compare=200,
                 num_inputs_agg=400,
                 **kwargs):
        super(DecomposableAttention, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab, embed_size)
        self.attend = Attend(num_inputs_attend, num_hiddens)
        self.compare = Compare(num_inputs_compare, num_hiddens)
        # æœ‰3ç§å¯èƒ½çš„è¾“å‡ºï¼šè•´æ¶µã€çŸ›ç›¾å’Œä¸­æ€§
        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)

    def forward(self, X):
        premises, hypotheses = X
        # A/Bçš„å½¢çŠ¶ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œåºåˆ—A/Bçš„è¯å…ƒæ•°ï¼Œembed_sizeï¼‰
        # f_A/f_Bçš„å½¢çŠ¶ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œåºåˆ—A/Bçš„è¯å…ƒæ•°ï¼Œnum_hiddensï¼‰
        A = self.embedding(premises)
        B = self.embedding(hypotheses)
        # betaçš„å½¢çŠ¶ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œåºåˆ—Açš„è¯å…ƒæ•°ï¼Œembed_sizeï¼‰ï¼Œ
        # æ„å‘³ç€åºåˆ—Bè¢«è½¯å¯¹é½åˆ°åºåˆ—Açš„æ¯ä¸ªè¯å…ƒ(betaçš„ç¬¬1ä¸ªç»´åº¦)
        # alphaçš„å½¢çŠ¶ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œåºåˆ—Bçš„è¯å…ƒæ•°ï¼Œembed_sizeï¼‰ï¼Œ
        # æ„å‘³ç€åºåˆ—Aè¢«è½¯å¯¹é½åˆ°åºåˆ—Bçš„æ¯ä¸ªè¯å…ƒ(alphaçš„ç¬¬1ä¸ªç»´åº¦)
        beta, alpha = self.attend(A, B)
        # å°†ä¸€ä¸ªåºåˆ—ä¸­çš„è¯å…ƒä¸ä¸è¯¥è¯å…ƒè½¯å¯¹é½çš„å¦ä¸€ä¸ªåºåˆ—è¿›è¡Œæ¯”è¾ƒã€‚
        # V_A: ï¼ˆæ‰¹é‡å¤§å°ï¼Œåºåˆ—Bçš„è¯å…ƒæ•°ï¼Œembed_size + embed_size ï¼‰
        # V_B: ï¼ˆæ‰¹é‡å¤§å°ï¼Œåºåˆ—Bçš„è¯å…ƒæ•°ï¼Œembed_size + embed_size ï¼‰
        V_A, V_B = self.compare(A, B, beta, alpha)
        # æˆ‘ä»¬æœ‰æœ‰ä¸¤ç»„æ¯”è¾ƒå‘é‡ ğ¯ğ´,ğ‘– ï¼ˆ ğ‘–=1,â€¦,ğ‘š ï¼‰å’Œ ğ¯ğµ,ğ‘— ï¼ˆ ğ‘—=1,â€¦,ğ‘› ï¼‰ã€‚
        # åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†èšåˆè¿™äº›ä¿¡æ¯ä»¥æ¨æ–­é€»è¾‘å…³ç³»ã€‚
        # ç„¶åå°†ä¸¤ä¸ªæ±‚å’Œç»“æœçš„è¿ç»“æä¾›ç»™å‡½æ•° â„ ï¼ˆä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼‰ï¼Œä»¥è·å¾—é€»è¾‘å…³ç³»çš„åˆ†ç±»ç»“æœ
        Y_hat = self.aggregate(V_A, V_B)
        return Y_hat


if __name__ == '__main__':
    X = (torch.ones(128, 50, dtype=int), torch.ones(128, 50, dtype=int))
    model = DecomposableAttention(1000, 100, 200)
    print(model)
    y = model(X)
